{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 自然語言處理 HW1 \n",
        "# 組員 : 110590450 歐佳昀 110590452 莊于潔\n",
        "## due 4/1,2024\n",
        "\n",
        "- Goal: Sentiment classification on open source datasets\n",
        "\n",
        "- Input: TSATC: Twitter Sentiment Analysis Training Corpus (to be detailed later)\n",
        "\n",
        "- Output: Training classifiers to classify the sentiment of tweets (to be detailed later)\n",
        "\n",
        "### Tasks\n",
        "- Performing sentiment classification on twitter data (as detailed in the following slides)\n",
        "\n",
        "- Data: an open dataset from Huggingface\n",
        "\n",
        "- You have to submit the classification output\n",
        "\n",
        "### Data: \n",
        "[TSATC: Twitter Sentiment Analysis Training Corpus] from Hugging Face\n",
        "1,578,627 tweets, about 15MB in size\n",
        "Available at:\n",
        "https://huggingface.co/datasets/carblacac/twitter-sentiment-analysis \n",
        "\n",
        "Format: \n",
        "Two text files consisting of lines of records\n",
        "Each record contains 2 columns: feeling, text\n",
        "\n",
        "To train a classifier using the training set in any programming language\n",
        "To test the classification result for the test set\n",
        "\n",
        "### Output format: \n",
        "Classification results\n",
        "- Precision\n",
        "- Recall\n",
        "- F-measure\n",
        "- Accuracy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "引入需要的庫"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "# plotting\n",
        "import seaborn as sns\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "# nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "# sklearn\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics import roc_curve, auc    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "分裝處理train、test資料集"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "data = pd.read_csv(\"data/train_150k.txt\", header=None, names=[\"target\", \"text\"],sep='\\t')\n",
        "data_test = pd.read_csv(\"data/test_62k.txt\", header=None, names=[\"target\", \"text\"],sep='\\t')\n",
        "\n",
        "data_pos = data[data['target'] == 1]\n",
        "data_neg = data[data['target'] == 0]\n",
        "dataset = pd.concat([data_pos, data_neg])\n",
        "\n",
        "\n",
        "dataset['text']=dataset['text'].str.lower()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "清除網站連結"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def cleaning_URLs(data):\n",
        "    return re.sub(r'((www\\.[^\\s]+)|(https?://[^\\s]+))',' ',data)\n",
        "dataset['text'] = dataset['text'].apply(lambda x: cleaning_URLs(x))\n",
        "data_test['text'] = data_test['text'].apply(lambda x: cleaning_URLs(x))\n",
        "# print(\"cleaning_URLs\\n\",dataset['text'].tail())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "去除標點符號、奇怪符號"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "import string\n",
        "english_punctuations = string.punctuation\n",
        "punctuations_list = english_punctuations\n",
        "\n",
        "def cleaning_punctuations(text):\n",
        "    translator = str.maketrans('', '', punctuations_list)\n",
        "    return text.translate(translator)\n",
        "dataset['text']= dataset['text'].apply(lambda x: cleaning_punctuations(x))\n",
        "data_test['text']= data_test['text'].apply(lambda x: cleaning_punctuations(x))\n",
        "# print(\"cleaning_punctuations\\n\",dataset['text'].tail())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "置換連續重複字元"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "def cleaning_repeating_char(text):\n",
        "    return re.sub(r'(.)\\1+', r'\\1\\1', text)\n",
        "dataset['text'] = dataset['text'].apply(lambda x: cleaning_repeating_char(x))\n",
        "data_test['text'] = data_test['text'].apply(lambda x: cleaning_repeating_char(x))\n",
        "# print(\"cleaning_repeating_char\\n\",dataset['text'].tail())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "清除數字"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "def cleaning_numbers(data):\n",
        "    return re.sub('[0-9]+', '', data)\n",
        "dataset['text'] = dataset['text'].apply(lambda x: cleaning_numbers(x))\n",
        "data_test['text'] = data_test['text'].apply(lambda x: cleaning_numbers(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "清除單一字元"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "def cleaning_single_c(data):\n",
        "    return re.sub(r'\\s+[a-zA-Z]\\s+', ' ', data)\n",
        "dataset['text'] = dataset['text'].apply(lambda x: cleaning_single_c(x))\n",
        "data_test['text'] = data_test['text'].apply(lambda x: cleaning_single_c(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "將詞替換成原型形式"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "def lemmatize_text(text):\n",
        "    # 初始化WordNetLemmatizer\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    # 將文本分詞並將每個單詞轉換為其原型形式\n",
        "    lemmatized_words = [lemmatizer.lemmatize(word) for word in text.split()]\n",
        "    # 將單詞列表重新組合為文本\n",
        "    lemmatized_text = ' '.join(lemmatized_words)\n",
        "    return lemmatized_text\n",
        "dataset['text'] = dataset['text'].apply(lambda x: lemmatize_text(x))\n",
        "data_test['text'] = data_test['text'].apply(lambda x: lemmatize_text(x))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "清除非英文詞"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "def cleaning_non_eng(data):\n",
        "    cleaned_data = re.sub(r'[^a-zA-Z\\s]', '', data)\n",
        "    return cleaned_data\n",
        "dataset['text'] = dataset['text'].apply(cleaning_non_eng)\n",
        "data_test['text'] = data_test['text'].apply(cleaning_non_eng)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "清除多餘空白"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "def cleaning_multi_space(data):\n",
        "    return re.sub(r'\\s+', ' ', data, flags=re.I)\n",
        "dataset['text'] = dataset['text'].apply(lambda x: cleaning_multi_space(x))\n",
        "data_test['text'] = data_test['text'].apply(lambda x: cleaning_multi_space(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "存檔查看整理後的train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset.to_csv(\"other_data/dataset.csv\",index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "設定停用字"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "stopwordlist = ['a','an','about', 'above', 'after', 'again', 'ain', 'all', 'am', 'an',\n",
        "             'and','any','are', 'as', 'at', 'be', 'because', 'been', 'before',\n",
        "             'being', 'below', 'between','both', 'by', 'can', 'd', 'did', 'do',\n",
        "             'does', 'doing', 'down', 'during', 'each','few', 'for', 'from',\n",
        "             'further', 'had', 'has', 'have', 'having', 'he', 'her', 'here',\n",
        "             'hers', 'herself', 'him', 'himself', 'his', 'how', 'i', 'if', 'in',\n",
        "             'into','is', 'it', 'its', 'itself', 'just', 'll', 'm', 'ma',\n",
        "             'me', 'more', 'most','my', 'myself', 'now', 'o', 'of', 'on', 'once',\n",
        "             'only', 'or', 'other', 'our', 'ours','ourselves', 'out', 'own', 're',\n",
        "             's', 'same', 'she', \"shes\", 'should', \"shouldve\",'so', 'some', 'such',\n",
        "             't', 'than', 'that', \"thatll\", 'the', 'their', 'theirs', 'them',\n",
        "             'themselves', 'then', 'there', 'these', 'they', 'this', 'those',\n",
        "             'through', 'to', 'too','under', 'until', 'up', 've', 'very', 'was',\n",
        "             'we', 'were', 'what', 'when', 'where','which','while', 'who', 'whom',\n",
        "             'why', 'will', 'with', 'won', 'y', 'you', \"youd\",\"youll\", \"youre\",\n",
        "             \"youve\", 'your', 'yours', 'yourself', 'yourselves','today','day','½t','im','go','<UNK>']\n",
        "            \n",
        "            #  'today','day','im','i m','go','get','got','time','morning','tomorrow','amp',###\n",
        "            #  'going','really','one','twitter','wa','like','ill','½s','thats','still','but',\n",
        "            #  'know','½t','make','see','ive','much','off']  ###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "查看詞雲圖"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "# fig, axes = plt.subplots(2, 1, figsize=(16, 8))\n",
        "\n",
        "# data_neg = dataset['text'][74968:]\n",
        "# wc_neg = WordCloud(stopwords=stopwordlist ,max_words=2000, width=1600, height=800, collocations=False).generate(\" \".join(data_neg))\n",
        "# axes[0].imshow(wc_neg, interpolation='bilinear')\n",
        "# axes[0].axis('off')\n",
        "# axes[0].set_title('Negative Words')  # 使用 set_title() 方法设置标题\n",
        "\n",
        "# data_pos = dataset['text'][:74968]\n",
        "# wc_pos = WordCloud(stopwords=stopwordlist ,max_words=2000, width=1600, height=800, collocations=False).generate(\" \".join(data_pos))\n",
        "# axes[1].imshow(wc_pos, interpolation='bilinear')\n",
        "# axes[1].axis('off')\n",
        "# axes[1].set_title('Positive Words')  # 使用 set_title() 方法设置标题\n",
        "\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "特徵向量化(TfidfVectorizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['unk'] not in stop_words.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No. of feature_words:  3500\n"
          ]
        }
      ],
      "source": [
        "dataset = dataset.sample(frac=1).reset_index(drop=True) #打亂，不知道，這樣好像會訓練的比較好\n",
        "dataset.to_csv(\"other_data/try.csv\",index= False)\n",
        "\n",
        "X_train = dataset.text\n",
        "y_train = dataset.target\n",
        "X_test = data_test.text\n",
        "y_test = data_test.target\n",
        "# vectoriser = TfidfVectorizer( min_df=7, max_df=0.8, max_features=5500,stop_words=stopwordlist)\n",
        "# vectoriser = TfidfVectorizer( min_df=7,stop_words=stopwordlist,max_features=2500)\n",
        "vectoriser = TfidfVectorizer( min_df=7,stop_words=stopwordlist,max_features=3500)\n",
        "# vectoriser = TfidfVectorizer( min_df=0.0001, stop_words=stopwordlist)\n",
        "vectoriser.fit(X_train)\n",
        "print('No. of feature_words: ', len(vectoriser.get_feature_names_out()))\n",
        "\n",
        "X_train = vectoriser.transform(X_train)\n",
        "X_test  = vectoriser.transform(X_test)\n",
        "\n",
        "# print('Features:', vectoriser.get_feature_names_out())\n",
        "feature_names_df = pd.DataFrame({'Feature Names': vectoriser.get_feature_names_out()})\n",
        "# 將DataFrame保存為CSV文件\n",
        "feature_names_df.to_csv('other_data/feature_names.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "可查看詳細confussion matrix(取消註解可以查看圖片)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "def model_Evaluate(model,name):\n",
        "    # Predict values for Test dataset\n",
        "\n",
        "    # y_pred = model.predict(X_test)\n",
        "    y_pred = model.predict(X_test.toarray())  # Convert to array\n",
        "    \n",
        "    # Print the evaluation metrics for the dataset.\n",
        "    print(classification_report(y_test, y_pred))\n",
        "    report = classification_report(y_test, y_pred, output_dict=True)\n",
        "    # Compute and plot the Confusion matrix\n",
        "    # cf_matrix = confusion_matrix(y_test, y_pred)\n",
        "    # categories = ['Negative','Positive']\n",
        "    # group_names = ['True Neg','False Pos', 'False Neg','True Pos']\n",
        "    # group_percentages = ['{0:.2%}'.format(value) for value in cf_matrix.flatten() / np.sum(cf_matrix)]\n",
        "    # labels = [f'{v1}n{v2}' for v1, v2 in zip(group_names,group_percentages)]\n",
        "    # labels = np.asarray(labels).reshape(2,2)\n",
        "\n",
        "    # TP = report['1']['precision'] * report['1']['support']\n",
        "    # FP = (1 - report['1']['precision']) * report['0']['support']\n",
        "    # FN = (1 - report['1']['recall']) * report['1']['support']    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.76      0.77     30969\n",
            "           1       0.77      0.79      0.78     31029\n",
            "\n",
            "    accuracy                           0.77     61998\n",
            "   macro avg       0.77      0.77      0.77     61998\n",
            "weighted avg       0.77      0.77      0.77     61998\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Create a Logistic Regression model\n",
        "logreg_model = LogisticRegression()\n",
        "\n",
        "# Train the model\n",
        "logreg_model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the model\n",
        "model_Evaluate(logreg_model, \"LogisticRegression\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "k-Nearest Neighbors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# # Create a k-NN model\n",
        "# knn_model = KNeighborsClassifier()\n",
        "\n",
        "# # Train the model\n",
        "# knn_model.fit(X_train, y_train)\n",
        "\n",
        "# # Evaluate the model\n",
        "# model_Evaluate(knn_model, \"KNeighborsClassifier\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from sklearn.svm import SVC\n",
        "\n",
        "# # Create a SVM model\n",
        "# SVM_model = SVC()\n",
        "\n",
        "# # Train the model\n",
        "# SVM_model.fit(X_train, y_train)\n",
        "\n",
        "# # Evaluate the model\n",
        "# model_Evaluate(SVM_model, \"SVC\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "高斯 Naïve Bayes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.71      0.68      0.69     30969\n",
            "           1       0.69      0.72      0.71     31029\n",
            "\n",
            "    accuracy                           0.70     61998\n",
            "   macro avg       0.70      0.70      0.70     61998\n",
            "weighted avg       0.70      0.70      0.70     61998\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "# Create a Gaussian Naive Bayes model\n",
        "gnb_model = GaussianNB()\n",
        "\n",
        "# Train the model\n",
        "gnb_model.fit(X_train.toarray(), y_train)  # GaussianNB requires array input\n",
        "\n",
        "# Evaluate the model\n",
        "model_Evaluate(gnb_model, \"GaussianNB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "BernoulliNB ->  白努力素葉貝斯分析"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      0.75      0.76     30969\n",
            "           1       0.76      0.78      0.77     31029\n",
            "\n",
            "    accuracy                           0.77     61998\n",
            "   macro avg       0.77      0.77      0.77     61998\n",
            "weighted avg       0.77      0.77      0.77     61998\n",
            "\n"
          ]
        }
      ],
      "source": [
        "BNBmodel = BernoulliNB()\n",
        "BNBmodel.fit(X_train, y_train)\n",
        "model_Evaluate(BNBmodel,'BernoulliNB')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "MultinomialNB ->  單純貝斯分析"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.75      0.77      0.76     30969\n",
            "           1       0.76      0.74      0.75     31029\n",
            "\n",
            "    accuracy                           0.76     61998\n",
            "   macro avg       0.76      0.76      0.76     61998\n",
            "weighted avg       0.76      0.76      0.76     61998\n",
            "\n"
          ]
        }
      ],
      "source": [
        "MNB_model = MultinomialNB()\n",
        "MNB_model.fit(X_train, y_train)\n",
        "model_Evaluate(MNB_model,\"MultinomialNB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "CountVectorizer  使用bag 進行向量提取"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['unk'] not in stop_words.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "X_train = dataset.text\n",
        "y_train = dataset.target\n",
        "X_test = data_test.text\n",
        "y_test = data_test.target\n",
        "\n",
        "\n",
        "# 创建词袋模型向量化器，并指定预处理函数\n",
        "# vectorizer = CountVectorizer(preprocessor=preprocess_text, ...)\n",
        "# vectorizer = CountVectorizer(stop_words=stopwordlist,min_df=10)\n",
        "vectorizer = CountVectorizer(stop_words=stopwordlist,max_features=3500)\n",
        "\n",
        "text = vectorizer.fit_transform(dataset['text'])\n",
        "X_test  = vectorizer.transform(data_test['text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "置換停用字 (無法提高正確率)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "# vectorizer = CountVectorizer(stop_words=stopwordlist,max_features=3500)\n",
        "\n",
        "# text = vectorizer.fit_transform(dataset['text'])\n",
        "# X_test  = vectorizer.transform(data_test['text'])\n",
        "\n",
        "# v = vectorizer.get_feature_names_out()\n",
        " \n",
        "# def preprocess_text(text):\n",
        "#     # 将未知单词替换为<UNK>\n",
        "#     return ' '.join(['<UNK>' if word not in v else word for word in text.split()])\n",
        "\n",
        "# X_test_preprocessed = [preprocess_text(text) for text in data_test.text]\n",
        "\n",
        "# X_train_vectorized = vectorizer.transform( dataset[\"text\"])\n",
        "# X_test_vectorized = vectorizer.transform(X_test_preprocessed)\n",
        "\n",
        "\n",
        "# text = X_train_vectorized\n",
        "# X_test = X_test_vectorized "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.75      0.78      0.76     30969\n",
            "           1       0.77      0.74      0.76     31029\n",
            "\n",
            "    accuracy                           0.76     61998\n",
            "   macro avg       0.76      0.76      0.76     61998\n",
            "weighted avg       0.76      0.76      0.76     61998\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 實例化(Instantiate) 這個 Naive Bayes Classifier\n",
        "MNB_model = MultinomialNB()\n",
        "# 把資料給它，讓這個model根據貝氏定理，去算那些機率。\n",
        "MNB_model.fit(text, y_train)\n",
        "model_Evaluate(MNB_model,\"MNB_model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      0.75      0.76     30969\n",
            "           1       0.76      0.78      0.77     31029\n",
            "\n",
            "    accuracy                           0.77     61998\n",
            "   macro avg       0.77      0.77      0.77     61998\n",
            "weighted avg       0.77      0.77      0.77     61998\n",
            "\n"
          ]
        }
      ],
      "source": [
        "BNBmodel = BernoulliNB()\n",
        "BNBmodel.fit(text, y_train)\n",
        "model_Evaluate(BNBmodel,\"BNBmodel\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.75      0.77     30969\n",
            "           1       0.76      0.80      0.78     31029\n",
            "\n",
            "    accuracy                           0.77     61998\n",
            "   macro avg       0.77      0.77      0.77     61998\n",
            "weighted avg       0.77      0.77      0.77     61998\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Create a Logistic Regression model\n",
        "logreg_model = LogisticRegression()\n",
        "\n",
        "# Train the model\n",
        "logreg_model.fit(text, y_train)\n",
        "\n",
        "# Evaluate the model\n",
        "model_Evaluate(logreg_model, \"LogisticRegression\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
